// å‘çº¯MCTSå­¦ä¹  - æ¨¡ä»¿å­¦ä¹ ï¼ˆImitation Learningï¼‰
use gomoku::az_mcts_rollout::MCTSWithRollout;
use gomoku::az_net::Connect4Trainer; // ç”¨åŸæ¥çš„ç½‘ç»œï¼Œåªæ˜¯è®­ç»ƒæ–¹å¼ä¸åŒ
use gomoku::connect4::Connect4;
use rand::Rng;
use tch::Tensor;

fn main() {
    println!("ğŸ“ å‘çº¯MCTSå­¦ä¹ è®­ç»ƒ\n");
    println!("ç­–ç•¥ï¼šè®©ç¥ç»ç½‘ç»œæ¨¡ä»¿å¼ºæ‰‹ï¼ˆçº¯MCTSï¼‰çš„èµ°æ³•");
    println!("ä¼˜åŠ¿ï¼šç›´æ¥å­¦ä¹ å¥½ç­–ç•¥ï¼Œä¸éœ€è¦è‡ªæˆ‘æ¢ç´¢\n");

    let mut trainer = Connect4Trainer::new(64, 0.001);

    let num_iterations = 50;  // å‡å°‘è¿­ä»£æ¬¡æ•°ï¼ˆæ•°æ®è´¨é‡æ›´é‡è¦ï¼‰
    let games_per_iter = 50;  // å¢åŠ æ¯è½®æ¸¸æˆæ•°
    let mcts_simulations = 200; // å¢åŠ MCTSæ¨¡æ‹Ÿæ¬¡æ•° - æ›´å¼ºçš„è€å¸ˆï¼
    let batch_size = 64;
    let train_epochs = 30;  // å¢åŠ è®­ç»ƒè½®æ•°

    println!("ğŸ“‹ é…ç½®:");
    println!("  ç½‘ç»œ: 5å±‚CNN + 64 filters");
    println!("  è€å¸ˆ: çº¯MCTS ({}æ¬¡æ¨¡æ‹Ÿï¼Œæ— ç½‘ç»œ)", mcts_simulations);
    println!("  æ•°æ®: æ¯è½®{}å±€æ¸¸æˆ", games_per_iter);
    println!("  è®­ç»ƒ: {}epochs per iteration\n", train_epochs);

    let mut all_data: Vec<(Vec<f32>, Vec<f32>, f32)> = Vec::new();
    let mut mcts = MCTSWithRollout::new(mcts_simulations, true);

    for iteration in 1..=num_iterations {
        println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        println!("ğŸ“Š è¿­ä»£ {}/{}", iteration, num_iterations);
        println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

        // ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼šè§‚å¯ŸMCTSæ€ä¹ˆä¸‹
        print!("  ğŸ® æ”¶é›†MCTSç¤ºèŒƒæ•°æ®... ");
        for _ in 0..games_per_iter {
            let game_data = collect_mcts_game(&mut mcts, &trainer);
            all_data.extend(game_data);
        }
        println!("âœ… æ•°æ®æ€»é‡: {}", all_data.len());

        // è®­ç»ƒç½‘ç»œå»æ¨¡ä»¿MCTS
        if all_data.len() >= batch_size {
            print!("  ğŸ¯ è®­ç»ƒç½‘ç»œæ¨¡ä»¿MCTS... ");
            let mut total_loss = 0.0;
            let mut count = 0;

            for _ in 0..train_epochs {
                // éšæœºé‡‡æ ·ä¸€ä¸ªbatch
                let mut rng = rand::thread_rng();
                let mut batch_boards: Vec<f32> = Vec::new();
                let mut batch_policies: Vec<f32> = Vec::new();
                let mut batch_values: Vec<f32> = Vec::new();

                for _ in 0..batch_size.min(all_data.len()) {
                    let idx = rng.gen_range(0..all_data.len());
                    let (board, policy, value) = &all_data[idx];
                    batch_boards.extend(board);
                    batch_policies.extend(policy);
                    batch_values.push(*value);
                }

                let boards_t = Tensor::f_from_slice(&batch_boards).unwrap().reshape(&[
                    batch_size as i64,
                    3,
                    6,
                    7,
                ]);
                let policies_t = Tensor::f_from_slice(&batch_policies)
                    .unwrap()
                    .reshape(&[batch_size as i64, 7]);
                let values_t = Tensor::f_from_slice(&batch_values)
                    .unwrap()
                    .reshape(&[batch_size as i64, 1]);

                let (_p_loss, _v_loss, t_loss) =
                    trainer.train_batch(&boards_t, &policies_t, &values_t);

                total_loss += t_loss;
                count += 1;
            }

            let avg_loss = total_loss / count as f64;
            println!("âœ… å¹³å‡loss: {:.4}", avg_loss);
        }

        // ä¿æŒæœ€è¿‘5000æ¡æ•°æ®
        if all_data.len() > 5000 {
            all_data.drain(0..all_data.len() - 5000);
        }

        // æ¯10è½®è¯„ä¼°ä¸€æ¬¡
        if iteration % 10 == 0 {
            println!("\n  ğŸ“Š è¯„ä¼°å­¦ä¹ æ•ˆæœ:");
            evaluate_student(&trainer, iteration);

            let filename = format!("connect4_imitation_iter_{}.pt", iteration);
            trainer.save(&filename).ok();
            println!("  ğŸ’¾ ä¿å­˜: {}\n", filename);
        }
    }

    println!("\nğŸ‰ è®­ç»ƒå®Œæˆï¼");
    println!("\nğŸ“Š æœ€ç»ˆè¯„ä¼°:");
    evaluate_student(&trainer, num_iterations);

    trainer.save("connect4_imitation_final.pt").ok();
    println!("\nğŸ’¾ æœ€ç»ˆæ¨¡å‹: connect4_imitation_final.pt");
}

// æ”¶é›†ä¸€å±€MCTSçš„ç¤ºèŒƒæ•°æ®
fn collect_mcts_game(
    mcts: &mut MCTSWithRollout,
    trainer: &Connect4Trainer,
) -> Vec<(Vec<f32>, Vec<f32>, f32)> {
    mcts.reset(); // é‡ç½®MCTSæ ‘ï¼Œé¿å…è·¨æ¸¸æˆæ±¡æŸ“
    let mut game = Connect4::new();
    let mut history = Vec::new();

    while !game.is_game_over() {
        // è®©MCTSå†³ç­–
        let policy = mcts.search(&game, &trainer.net);

        // é€‰æ‹©æœ€ä½³åŠ¨ä½œï¼ˆä¸ç”¨æ¸©åº¦ï¼Œç›´æ¥é€‰æœ€å¥½çš„ï¼‰
        let action = policy
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .map(|(i, _)| i)
            .unwrap();

        history.push((game.to_tensor(), policy));

        if game.play(action).is_err() {
            break;
        }
    }

    // æ ‡æ³¨ç»“æœ
    let final_value = if game.is_game_over() && game.winner().is_some() {
        1.0
    } else {
        0.0
    };

    // ç”Ÿæˆè®­ç»ƒæ•°æ®
    let mut training_data = Vec::new();
    for (i, (board, policy)) in history.iter().enumerate() {
        // ä»å½“å‰ç©å®¶è§†è§’
        let value = if i % 2 == (history.len() - 1) % 2 {
            final_value
        } else {
            -final_value
        };

        training_data.push((board.clone(), policy.clone(), value));
    }

    training_data
}

// è¯„ä¼°å­¦ç”Ÿç½‘ç»œ
fn evaluate_student(trainer: &Connect4Trainer, _iteration: usize) {
    let mut rng = rand::thread_rng();

    // æµ‹è¯•vséšæœº
    let mut wins = 0;
    let num_games = 20;

    for _ in 0..num_games {
        let mut game = Connect4::new();
        let student_first = rng.gen_bool(0.5);

        while !game.is_game_over() {
            let is_student_turn = (game.current_player() == 1) == student_first;

            if is_student_turn {
                // å­¦ç”Ÿç½‘ç»œå†³ç­–
                let board_t = Tensor::f_from_slice(&game.to_tensor())
                    .unwrap()
                    .reshape(&[1, 3, 6, 7]);
                let (policy, _) = trainer.net.predict(&board_t);

                let mut policy_vec = vec![0.0f32; 7];
                policy.view([7i64]).copy_data(&mut policy_vec, 7);

                // Softmax
                let max_val = policy_vec.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
                let exp_vals: Vec<f32> = policy_vec.iter().map(|&x| (x - max_val).exp()).collect();
                let sum: f32 = exp_vals.iter().sum();
                let probs: Vec<f32> = exp_vals.iter().map(|&x| x / sum).collect();

                let action = probs
                    .iter()
                    .enumerate()
                    .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                    .map(|(i, _)| i)
                    .unwrap();

                if game.play(action).is_err() {
                    break;
                }
            } else {
                // éšæœºå¯¹æ‰‹
                let valid = game.legal_moves();
                if valid.is_empty() {
                    break;
                }
                let action = valid[rng.gen_range(0..valid.len())];
                game.play(action).unwrap();
            }
        }

        if game.winner().is_some() {
            let winner = game.winner().unwrap();
            if (winner == 1) == student_first {
                wins += 1;
            }
        }
    }

    println!(
        "    vs éšæœº: {}/{}  ({:.0}%)",
        wins,
        num_games,
        wins as f64 / num_games as f64 * 100.0
    );

    // æµ‹è¯•vså¼±MCTS
    let mut wins_mcts = 0;
    let mut mcts_weak = MCTSWithRollout::new(30, true);

    for _ in 0..10 {
        mcts_weak.reset(); // é‡ç½®MCTSæ ‘
        let mut game = Connect4::new();
        let student_first = rng.gen_bool(0.5);

        while !game.is_game_over() {
            let is_student_turn = (game.current_player() == 1) == student_first;

            if is_student_turn {
                // å­¦ç”Ÿç½‘ç»œ
                let board_t = Tensor::f_from_slice(&game.to_tensor())
                    .unwrap()
                    .reshape(&[1, 3, 6, 7]);
                let (policy, _) = trainer.net.predict(&board_t);

                let mut policy_vec = vec![0.0f32; 7];
                policy.view([7i64]).copy_data(&mut policy_vec, 7);

                let max_val = policy_vec.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
                let exp_vals: Vec<f32> = policy_vec.iter().map(|&x| (x - max_val).exp()).collect();
                let sum: f32 = exp_vals.iter().sum();
                let probs: Vec<f32> = exp_vals.iter().map(|&x| x / sum).collect();

                let action = probs
                    .iter()
                    .enumerate()
                    .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                    .map(|(i, _)| i)
                    .unwrap();

                if game.play(action).is_err() {
                    break;
                }
            } else {
                // å¼±MCTS (30æ¬¡æ¨¡æ‹Ÿ)
                let policy = mcts_weak.search(&game, &trainer.net);
                let action = policy
                    .iter()
                    .enumerate()
                    .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                    .map(|(i, _)| i)
                    .unwrap();

                if game.play(action).is_err() {
                    break;
                }
            }
        }

        if game.winner().is_some() {
            let winner = game.winner().unwrap();
            if (winner == 1) == student_first {
                wins_mcts += 1;
            }
        }
    }

    println!(
        "    vs å¼±MCTS(30æ¨¡æ‹Ÿ): {}/10  ({:.0}%)",
        wins_mcts,
        wins_mcts as f64 * 10.0
    );
}
