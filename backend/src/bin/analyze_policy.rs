// åˆ†æç½‘ç»œå­¦åˆ°çš„policyè´¨é‡
use gomoku::az_mcts_rollout::MCTSWithRollout;
use gomoku::az_net::Connect4Trainer;
use gomoku::connect4::Connect4;
use tch::Tensor;

fn main() {
    println!("ğŸ” åˆ†æPolicyè´¨é‡\n");

    let mut trainer = Connect4Trainer::new(64, 0.0003);
    if let Err(_) = trainer.load("connect4_model_iter_30.pt") {
        println!("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹");
    } else {
        println!("âœ… åŠ è½½äº†è®­ç»ƒåçš„æ¨¡å‹");
    }

    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("æµ‹è¯•1: åˆå§‹å±€é¢ - ç½‘ç»œPolicy vs çº¯MCTS Policy");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let game = Connect4::new();

    // ç½‘ç»œçš„policy
    let board_tensor = Tensor::f_from_slice(&game.to_tensor())
        .unwrap()
        .reshape(&[1, 3, 6, 7]);
    let (policy, _value) = trainer.net.predict(&board_tensor);
    let mut policy_vec = vec![0.0f32; 7];
    policy.view([7]).copy_data(&mut policy_vec, 7);

    // è½¬æ¢ä¸ºæ¦‚ç‡
    let policy_probs: Vec<f32> = policy_vec.iter().map(|&x| x.exp()).collect();
    let sum: f32 = policy_probs.iter().sum();
    let policy_probs: Vec<f32> = policy_probs.iter().map(|&x| x / sum).collect();

    println!("ç½‘ç»œPolicy (åˆå§‹å±€é¢):");
    for (i, &p) in policy_probs.iter().enumerate() {
        println!("  åˆ—{}: {:.1}%", i, p * 100.0);
    }

    // çº¯MCTSçš„policy (ä½œä¸ºbaseline)
    println!("\nçº¯MCTS Policy (50æ¨¡æ‹Ÿ):");
    let mut pure_mcts = MCTSWithRollout::new(50, true);
    let mcts_probs = pure_mcts.search(&game, &trainer.net);
    for (i, &p) in mcts_probs.iter().enumerate() {
        println!("  åˆ—{}: {:.1}%", i, p * 100.0);
    }

    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("æµ‹è¯•2: ç®€å•å¨èƒå±€é¢");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    // åˆ›å»ºä¸€ä¸ªç®€å•çš„å¨èƒï¼šXæœ‰3ä¸ªè¿åœ¨ä¸€èµ·
    let mut game2 = Connect4::new();
    game2.play(3).unwrap(); // X
    game2.play(3).unwrap(); // O
    game2.play(2).unwrap(); // X
    game2.play(2).unwrap(); // O
    game2.play(4).unwrap(); // X
    game2.play(4).unwrap(); // O
                            // ç°åœ¨Xå¦‚æœä¸‹åˆ—1æˆ–åˆ—5å°±èƒ½èµ¢

    println!("å½“å‰å±€é¢:");
    game2.print();
    println!("Xåº”è¯¥ä¸‹åˆ—1æˆ–åˆ—5æ¥è¿4èµ¢");

    let board_tensor = Tensor::f_from_slice(&game2.to_tensor())
        .unwrap()
        .reshape(&[1, 3, 6, 7]);
    let (policy, _value) = trainer.net.predict(&board_tensor);
    let mut policy_vec = vec![0.0f32; 7];
    policy.view([7]).copy_data(&mut policy_vec, 7);

    let policy_probs: Vec<f32> = policy_vec.iter().map(|&x| x.exp()).collect();
    let sum: f32 = policy_probs.iter().sum();
    let policy_probs: Vec<f32> = policy_probs.iter().map(|&x| x / sum).collect();

    println!("\nç½‘ç»œPolicy:");
    for (i, &p) in policy_probs.iter().enumerate() {
        let marker = if i == 1 || i == 5 { " â† æ­£ç¡®" } else { "" };
        println!("  åˆ—{}: {:.1}%{}", i, p * 100.0, marker);
    }

    let best_col = policy_probs
        .iter()
        .enumerate()
        .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
        .map(|(i, _)| i)
        .unwrap();

    if best_col == 1 || best_col == 5 {
        println!("\nâœ… ç½‘ç»œé€‰æ‹©äº†æ­£ç¡®çš„åˆ—{}", best_col);
    } else {
        println!("\nâŒ ç½‘ç»œé€‰æ‹©äº†é”™è¯¯çš„åˆ—{} (åº”è¯¥æ˜¯1æˆ–5)", best_col);
    }

    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("æµ‹è¯•3: å¿…é¡»é˜²å®ˆçš„å±€é¢");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut game3 = Connect4::new();
    game3.play(3).unwrap(); // X
    game3.play(2).unwrap(); // O
    game3.play(3).unwrap(); // X
    game3.play(2).unwrap(); // O
    game3.play(3).unwrap(); // X
    game3.play(2).unwrap(); // O
                            // ç°åœ¨Oå¦‚æœä¸åœ¨åˆ—3é˜²å®ˆï¼ŒXä¸‹ä¸€æ­¥åˆ—3å°±èµ¢äº†

    println!("å½“å‰å±€é¢:");
    game3.print();
    println!("Oå¿…é¡»åœ¨åˆ—3é˜²å®ˆï¼");

    let board_tensor = Tensor::f_from_slice(&game3.to_tensor())
        .unwrap()
        .reshape(&[1, 3, 6, 7]);
    let (policy, _value) = trainer.net.predict(&board_tensor);
    let mut policy_vec = vec![0.0f32; 7];
    policy.view([7]).copy_data(&mut policy_vec, 7);

    let policy_probs: Vec<f32> = policy_vec.iter().map(|&x| x.exp()).collect();
    let sum: f32 = policy_probs.iter().sum();
    let policy_probs: Vec<f32> = policy_probs.iter().map(|&x| x / sum).collect();

    println!("\nç½‘ç»œPolicy:");
    for (i, &p) in policy_probs.iter().enumerate() {
        let marker = if i == 3 { " â† æ­£ç¡®(é˜²å®ˆ)" } else { "" };
        println!("  åˆ—{}: {:.1}%{}", i, p * 100.0, marker);
    }

    let best_col = policy_probs
        .iter()
        .enumerate()
        .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
        .map(|(i, _)| i)
        .unwrap();

    if best_col == 3 {
        println!("\nâœ… ç½‘ç»œæ­£ç¡®é˜²å®ˆ");
    } else {
        println!("\nâŒ ç½‘ç»œæ²¡æœ‰é˜²å®ˆåˆ—3ï¼Œé€‰æ‹©äº†åˆ—{}", best_col);
    }

    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“Š ç»“è®º");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("å¦‚æœç½‘ç»œåœ¨ç®€å•å±€é¢éƒ½æ— æ³•è¯†åˆ«æ˜æ˜¾çš„èµ¢æ³•/é˜²å®ˆï¼Œ");
    println!("è¯´æ˜policyå­¦ä¹ æœ‰é—®é¢˜ã€‚å¯èƒ½çš„åŸå› :");
    println!("  1. è®­ç»ƒæ•°æ®å¤ªå°‘ï¼ˆåªæœ‰900å±€ï¼‰");
    println!("  2. ç½‘ç»œå®¹é‡å¤ªå°ï¼ˆåªæœ‰3å±‚ï¼‰");
    println!("  3. MCTSæ¨¡æ‹Ÿå¤ªå°‘å¯¼è‡´è®­ç»ƒæ•°æ®è´¨é‡å·®");
}
