use gomoku::az_mcts_rollout::MCTSWithRollout;
use gomoku::az_net::Connect4Net;
use gomoku::az_resnet::Connect4ResNetTrainer;
use gomoku::connect4::Connect4;
use rand::Rng;
use tch::{nn, Device, Tensor};

// åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„Connect4Netæ¥å…¼å®¹MCTSæ¥å£ï¼ˆå®é™…MCTSç”¨rolloutä¸éœ€è¦ç½‘ç»œï¼‰
fn create_dummy_net(device: Device) -> Connect4Net {
    let vs = nn::VarStore::new(device);
    Connect4Net::new(&vs.root(), 64)
}

fn main() {
    println!("ğŸš€ ç»ˆæä¼˜åŒ–è®­ç»ƒ - ResNet + GPU + æ¨¡ä»¿å­¦ä¹ \n");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // è¶…å‚æ•° - åŸºäºæ‰€æœ‰ç»éªŒè°ƒä¼˜
    let num_filters = 128; // å¢åŠ åˆ°128 (vs 64)
    let num_residual_blocks = 10; // 10å±‚æ®‹å·®ç½‘ç»œ
    let learning_rate = 0.001;
    let mcts_simulations = 200; // æ›´å¼ºçš„MCTSè€å¸ˆ
    let games_per_iter = 50; // æ›´å¤šæ¸¸æˆæ•°æ®
    let batch_size = 128; // GPUæ”¯æŒæ›´å¤§batch
    let train_epochs = 30; // æ›´å¤šè®­ç»ƒè½®æ¬¡
    let num_iterations = 50; // 50æ¬¡è¿­ä»£
    let buffer_size = 20000; // æ›´å¤§çš„ç¼“å†²åŒº

    println!("ğŸ“‹ è¶…å‚æ•°é…ç½®:");
    println!(
        "  ç½‘ç»œ: ResNet-{} with {} filters",
        num_residual_blocks, num_filters
    );
    println!("  å­¦ä¹ ç‡: {}", learning_rate);
    println!("  MCTSæ¨¡æ‹Ÿ: {}æ¬¡ (çº¯rolloutï¼Œä¸ç”¨ç½‘ç»œ)", mcts_simulations);
    println!("  æ¯è½®æ¸¸æˆ: {}å±€", games_per_iter);
    println!("  æ‰¹æ¬¡å¤§å°: {}", batch_size);
    println!("  è®­ç»ƒè½®æ¬¡: {} epochs/iter", train_epochs);
    println!("  æ•°æ®ç¼“å†²: {} æ¡", buffer_size);
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // åˆ›å»ºtrainer (è‡ªåŠ¨ä½¿ç”¨GPU)
    let mut trainer = Connect4ResNetTrainer::new(num_filters, num_residual_blocks, learning_rate);

    // åˆ›å»ºä¸€ä¸ªdummy netç”¨äºMCTS (å®é™…ä¸ŠMCTSç”¨rolloutï¼Œä¸éœ€è¦ç½‘ç»œ)
    let dummy_net = create_dummy_net(trainer.device());

    // æ•°æ®ç¼“å†²åŒº
    let mut all_data: Vec<(Vec<f32>, Vec<f32>, f32)> = Vec::new();
    let mut mcts = MCTSWithRollout::new(mcts_simulations, true);

    // æœ€ä½³æ¨¡å‹è¿½è¸ª
    let mut best_win_rate = 0.0;
    let mut patience = 0;
    let max_patience = 10;

    for iteration in 1..=num_iterations {
        println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        println!("ğŸ“Š è¿­ä»£ {}/{}", iteration, num_iterations);
        println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

        // 1. æ”¶é›†MCTSç¤ºèŒƒæ•°æ®
        print!("  ğŸ® æ”¶é›†MCTSç¤ºèŒƒæ•°æ®... ");
        for _ in 0..games_per_iter {
            let game_data = collect_mcts_game(&mut mcts, &trainer, &dummy_net);
            all_data.extend(game_data);
        }

        // ä¿æŒç¼“å†²åŒºå¤§å°
        if all_data.len() > buffer_size {
            all_data.drain(0..all_data.len() - buffer_size);
        }
        println!("âœ… æ•°æ®æ€»é‡: {}", all_data.len());

        // 2. è®­ç»ƒç½‘ç»œ
        if all_data.len() >= batch_size {
            print!("  ğŸ¯ è®­ç»ƒç½‘ç»œ... ");
            let mut total_loss = 0.0;
            let mut count = 0;

            for _ in 0..train_epochs {
                let mut rng = rand::thread_rng();
                let mut batch_boards: Vec<f32> = Vec::new();
                let mut batch_policies: Vec<f32> = Vec::new();
                let mut batch_values: Vec<f32> = Vec::new();

                for _ in 0..batch_size.min(all_data.len()) {
                    let idx = rng.gen_range(0..all_data.len());
                    let (board, policy, value) = &all_data[idx];
                    batch_boards.extend(board);
                    batch_policies.extend(policy);
                    batch_values.push(*value);
                }

                let device = trainer.device();
                let boards_t = Tensor::f_from_slice(&batch_boards)
                    .unwrap()
                    .reshape(&[batch_size as i64, 3, 6, 7])
                    .to_device(device);
                let policies_t = Tensor::f_from_slice(&batch_policies)
                    .unwrap()
                    .reshape(&[batch_size as i64, 7])
                    .to_device(device);
                let values_t = Tensor::f_from_slice(&batch_values)
                    .unwrap()
                    .reshape(&[batch_size as i64, 1])
                    .to_device(device);

                let (_p_loss, _v_loss, t_loss) =
                    trainer.train_batch(&boards_t, &policies_t, &values_t);

                total_loss += t_loss;
                count += 1;
            }

            let avg_loss = total_loss / count as f64;
            println!("âœ… å¹³å‡loss: {:.4}", avg_loss);
        }

        // 3. æ¯5è½®è¯„ä¼°ä¸€æ¬¡
        if iteration % 5 == 0 {
            println!("\n  ğŸ“Š è¯„ä¼°å­¦ä¹ æ•ˆæœ:");

            let (win_vs_random, win_vs_mcts) = evaluate_student(&trainer, iteration);

            println!("    vs éšæœº: {:.0}%", win_vs_random * 100.0);
            println!("    vs å¼±MCTS(50æ¨¡æ‹Ÿ): {:.0}%", win_vs_mcts * 100.0);

            // ä¿å­˜æ¨¡å‹
            let model_path = format!("connect4_resnet_iter_{}.pt", iteration);
            trainer.save(&model_path).unwrap();
            println!("  ğŸ’¾ ä¿å­˜: {}", model_path);

            // æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
            if win_vs_random > best_win_rate {
                best_win_rate = win_vs_random;
                patience = 0;
                trainer.save("connect4_resnet_best.pt").unwrap();
                println!("  ğŸŒŸ æ–°çš„æœ€ä½³æ¨¡å‹ï¼Win rate: {:.0}%", best_win_rate * 100.0);
            } else {
                patience += 1;
                println!("  â³ æ— æ”¹è¿› ({}/{})", patience, max_patience);
            }

            // Early stopping
            if patience >= max_patience {
                println!("\nâš ï¸  {} è½®æ— æ”¹è¿›ï¼Œè§¦å‘æ—©åœ", max_patience);
                break;
            }

            println!();
        }
    }

    println!("\nğŸ‰ è®­ç»ƒå®Œæˆï¼");
    println!("\nğŸ“Š æœ€ç»ˆè¯„ä¼°:");
    let (win_vs_random, win_vs_mcts) = evaluate_student(&trainer, 999);
    println!("    vs éšæœº: {:.0}%", win_vs_random * 100.0);
    println!("    vs å¼±MCTS(50æ¨¡æ‹Ÿ): {:.0}%", win_vs_mcts * 100.0);

    trainer.save("connect4_resnet_final.pt").unwrap();
    println!("\nğŸ’¾ æœ€ç»ˆæ¨¡å‹: connect4_resnet_final.pt");
    println!(
        "ğŸ’¾ æœ€ä½³æ¨¡å‹: connect4_resnet_best.pt (Win rate: {:.0}%)",
        best_win_rate * 100.0
    );
}

fn collect_mcts_game(
    mcts: &mut MCTSWithRollout,
    _trainer: &Connect4ResNetTrainer,
    dummy_net: &Connect4Net,
) -> Vec<(Vec<f32>, Vec<f32>, f32)> {
    mcts.reset();
    let mut game = Connect4::new();
    let mut history = Vec::new();

    while !game.is_game_over() {
        // ä½¿ç”¨çº¯rollout MCTSï¼ˆä¸éœ€è¦ç½‘ç»œè¯„ä¼°ï¼‰
        let policy = mcts.search(&game, dummy_net);

        let action = policy
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .map(|(i, _)| i)
            .unwrap();

        history.push((game.to_tensor(), policy));

        if game.play(action).is_err() {
            break;
        }
    }

    let final_value = if game.is_game_over() && game.winner().is_some() {
        1.0
    } else {
        0.0
    };

    let mut training_data = Vec::new();
    for (i, (board, policy)) in history.iter().enumerate() {
        let value = if i % 2 == (history.len() - 1) % 2 {
            final_value
        } else {
            -final_value
        };

        training_data.push((board.clone(), policy.clone(), value));
    }

    training_data
}

fn evaluate_student(trainer: &Connect4ResNetTrainer, _iteration: usize) -> (f64, f64) {
    let mut rng = rand::thread_rng();
    let device = trainer.device();

    // åˆ›å»ºdummy netç”¨äºMCTS
    let dummy_net = create_dummy_net(device);

    // æµ‹è¯• vs éšæœº
    let mut wins = 0;
    let num_games = 30;

    for _ in 0..num_games {
        let mut game = Connect4::new();
        let student_first = rng.gen_bool(0.5);

        while !game.is_game_over() {
            let is_student_turn = (game.current_player() == 1) == student_first;

            if is_student_turn {
                let board_t = Tensor::f_from_slice(&game.to_tensor())
                    .unwrap()
                    .reshape(&[1, 3, 6, 7])
                    .to_device(device);
                let (policy, _) = trainer.net.predict(&board_t);

                let mut policy_vec = vec![0.0f32; 7];
                policy.view([7i64]).copy_data(&mut policy_vec, 7);

                let max_val = policy_vec.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
                let exp_vals: Vec<f32> = policy_vec.iter().map(|&x| (x - max_val).exp()).collect();
                let sum: f32 = exp_vals.iter().sum();
                let probs: Vec<f32> = exp_vals.iter().map(|&x| x / sum).collect();

                let action = probs
                    .iter()
                    .enumerate()
                    .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                    .map(|(i, _)| i)
                    .unwrap();

                if game.play(action).is_err() {
                    break;
                }
            } else {
                let valid = game.legal_moves();
                if valid.is_empty() {
                    break;
                }
                let action = valid[rng.gen_range(0..valid.len())];
                game.play(action).unwrap();
            }
        }

        if game.winner().is_some() {
            let winner = game.winner().unwrap();
            if (winner == 1) == student_first {
                wins += 1;
            }
        }
    }

    let win_rate_random = wins as f64 / num_games as f64;

    // æµ‹è¯• vs å¼±MCTS
    let mut wins_mcts = 0;
    let mut mcts_weak = MCTSWithRollout::new(50, true);

    for _ in 0..20 {
        mcts_weak.reset();
        let mut game = Connect4::new();
        let student_first = rng.gen_bool(0.5);

        while !game.is_game_over() {
            let is_student_turn = (game.current_player() == 1) == student_first;

            if is_student_turn {
                let board_t = Tensor::f_from_slice(&game.to_tensor())
                    .unwrap()
                    .reshape(&[1, 3, 6, 7])
                    .to_device(device);
                let (policy, _) = trainer.net.predict(&board_t);

                let mut policy_vec = vec![0.0f32; 7];
                policy.view([7i64]).copy_data(&mut policy_vec, 7);

                let max_val = policy_vec.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
                let exp_vals: Vec<f32> = policy_vec.iter().map(|&x| (x - max_val).exp()).collect();
                let sum: f32 = exp_vals.iter().sum();
                let probs: Vec<f32> = exp_vals.iter().map(|&x| x / sum).collect();

                let action = probs
                    .iter()
                    .enumerate()
                    .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                    .map(|(i, _)| i)
                    .unwrap();

                if game.play(action).is_err() {
                    break;
                }
            } else {
                let policy = mcts_weak.search(&game, &dummy_net);
                let action = policy
                    .iter()
                    .enumerate()
                    .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                    .map(|(i, _)| i)
                    .unwrap();

                if game.play(action).is_err() {
                    break;
                }
            }
        }

        if game.winner().is_some() {
            let winner = game.winner().unwrap();
            if (winner == 1) == student_first {
                wins_mcts += 1;
            }
        }
    }

    let win_rate_mcts = wins_mcts as f64 / 20.0;

    (win_rate_random, win_rate_mcts)
}
