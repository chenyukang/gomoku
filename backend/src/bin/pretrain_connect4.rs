use gomoku::connect4::Connect4;
use gomoku::az_mcts_rollout::MCTSWithRollout;
use gomoku::az_trainer::{AlphaZeroTrainer, TrainingSample};
use gomoku::az_net::Connect4Net;
use std::convert::TryFrom;
use tch::{nn, Device};

fn main() {
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“ Connect4 ç›‘ç£å­¦ä¹ é¢„è®­ç»ƒ");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    // é…ç½®å‚æ•°
    let num_filters = 128;
    let mcts_simulations = 200; // MCTSæ¨¡æ‹Ÿæ¬¡æ•° (é™ä½ä»¥åŠ å¿«é€Ÿåº¦)
    let num_games = 100;        // ç”Ÿæˆ100å±€æ¸¸æˆæ•°æ® (é™ä½ä»¥åŠ å¿«é€Ÿåº¦)
    let batch_size = 64;
    let train_epochs = 20;      // è®­ç»ƒ20è½® (é™ä½ä»¥åŠ å¿«æµ‹è¯•)
    let learning_rate = 0.01;   // ç›‘ç£å­¦ä¹ å¯ä»¥ç”¨æ›´å¤§çš„å­¦ä¹ ç‡

    println!("ğŸ“‹ é…ç½®:");
    println!("  ç½‘ç»œ: {} filters", num_filters);
    println!("  æ•°æ®ç”Ÿæˆ: {} å±€æ¸¸æˆï¼ŒMCTS {} æ¨¡æ‹Ÿï¼ˆä½¿ç”¨rolloutï¼‰", num_games, mcts_simulations);
    println!("  è®­ç»ƒ: {} epochs, batch_size={}, lr={}", train_epochs, batch_size, learning_rate);
    println!();

    // 1. ç”Ÿæˆè®­ç»ƒæ•°æ®
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“Š é˜¶æ®µ1: ä½¿ç”¨MCTS+Rolloutç”Ÿæˆè®­ç»ƒæ•°æ®");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let training_data = generate_expert_data(num_games, mcts_simulations);
    
    println!("\nâœ… æ•°æ®ç”Ÿæˆå®Œæˆ:");
    println!("  æ€»æ ·æœ¬æ•°: {}", training_data.len());
    println!("  å¹³å‡æ¯å±€: {:.1} æ­¥", training_data.len() as f32 / num_games as f32);

    // 2. åˆ›å»ºå¹¶è®­ç»ƒç½‘ç»œ
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ§  é˜¶æ®µ2: ç›‘ç£å­¦ä¹ è®­ç»ƒç¥ç»ç½‘ç»œ");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut trainer = AlphaZeroTrainer::new(
        num_filters,
        learning_rate,
        5000, // replay_buffer_size
        mcts_simulations as u32,
    );

    // å°†æ•°æ®åŠ å…¥è®­ç»ƒå™¨
    println!("ğŸ“¥ åŠ è½½è®­ç»ƒæ•°æ®åˆ°replay buffer...");
    for sample in training_data {
        trainer.add_sample(sample);
    }

    // ç›‘ç£å­¦ä¹ è®­ç»ƒ
    println!("ğŸ¯ å¼€å§‹ç›‘ç£å­¦ä¹ è®­ç»ƒ...\n");
    
    for epoch in 0..train_epochs {
        let num_batches = trainer.replay_buffer_size().max(batch_size) / batch_size;
        
        // è®­ç»ƒä¸€ä¸ªepoch
        trainer.train(batch_size, num_batches);
        
        // æ¯5ä¸ªepochè¯„ä¼°ä¸€æ¬¡
        if (epoch + 1) % 5 == 0 {
            println!("\nğŸ“Š Epoch {}/{} è¯„ä¼°:", epoch + 1, train_epochs);
            
            // å¿«é€Ÿè¯„ä¼°
            let win_rate = evaluate_model(&trainer, 20);
            println!("  vs éšæœº: {:.1}%", win_rate * 100.0);
            
            // æ£€æŸ¥policyåå·®
            check_policy_bias(&trainer);
            
            // ä¿å­˜æ£€æŸ¥ç‚¹
            let checkpoint_path = format!("connect4_pretrain_epoch_{}.pt", epoch + 1);
            if let Err(e) = trainer.save_model(&checkpoint_path) {
                eprintln!("  âš ï¸  ä¿å­˜å¤±è´¥: {}", e);
            } else {
                println!("  ğŸ’¾ å·²ä¿å­˜: {}", checkpoint_path);
            }
            println!();
        }
    }

    // 3. æœ€ç»ˆè¯„ä¼°
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“Š æœ€ç»ˆè¯„ä¼°");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("æµ‹è¯• vs éšæœºç©å®¶ (100å±€):");
    let final_win_rate = evaluate_model(&trainer, 100);
    println!("  èƒœç‡: {:.1}%\n", final_win_rate * 100.0);

    // æ£€æŸ¥policyåˆ†å¸ƒ
    check_policy_bias(&trainer);

    // ä¿å­˜æœ€ç»ˆæ¨¡å‹
    let final_path = "connect4_pretrained.pt";
    if let Err(e) = trainer.save_model(final_path) {
        eprintln!("âš ï¸  ä¿å­˜æœ€ç»ˆæ¨¡å‹å¤±è´¥: {}", e);
    } else {
        println!("\nğŸ‰ é¢„è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {}", final_path);
        println!("\nğŸ’¡ ä¸‹ä¸€æ­¥: ä½¿ç”¨æ­¤æ¨¡å‹ä½œä¸ºèµ·ç‚¹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ");
        println!("   1. è½¬æ¢æ¨¡å‹: python3 convert_model_v2.py {} connect4_pretrained_converted.pt", final_path);
        println!("   2. æµ‹è¯•æ¨¡å‹: å¤åˆ¶åˆ°clientç›®å½•å¹¶åœ¨ç½‘é¡µä¸­æµ‹è¯•");
    }
}

/// ä½¿ç”¨MCTS+Rolloutç”Ÿæˆä¸“å®¶çº§è®­ç»ƒæ•°æ®ï¼ˆä¸ä¾èµ–ç¥ç»ç½‘ç»œï¼‰
fn generate_expert_data(num_games: usize, mcts_sims: usize) -> Vec<TrainingSample> {
    let mut all_samples = Vec::new();
    
    // åˆ›å»ºä¸€ä¸ªdummyç½‘ç»œï¼ˆå› ä¸ºMCTSWithRollout signatureéœ€è¦ï¼Œä½†use_rollout=trueæ—¶ä¸ä¼šçœŸæ­£ä½¿ç”¨ï¼‰
    let device = Device::cuda_if_available();
    let vs = nn::VarStore::new(device);
    let dummy_net = Connect4Net::new(&vs.root(), 64); // å°ç½‘ç»œï¼Œä¸ä¼šè¢«ç”¨åˆ°
    
    for game_idx in 0..num_games {
        if (game_idx + 1) % 20 == 0 {
            println!("  å®Œæˆ {}/{} å±€", game_idx + 1, num_games);
        }
        
        let mut game = Connect4::new();
        let mut history = Vec::new();
        
        // ä½¿ç”¨MCTS+Rolloutè‡ªå¯¹å¼ˆï¼ˆuse_rollout=trueè¡¨ç¤ºç”¨rolloutï¼Œä¸ç”¨ç½‘ç»œï¼‰
        while !game.is_game_over() {
            let mut mcts = MCTSWithRollout::new(mcts_sims as u32, true); // true = ä½¿ç”¨rollout
            
            // æ‰§è¡ŒMCTSæœç´¢ï¼ˆè™½ç„¶ä¼ å…¥ç½‘ç»œï¼Œä½†use_rollout=trueæ—¶ä¸ä¼šä½¿ç”¨ï¼‰
            let policy = mcts.search(&game, &dummy_net);
            
            // ä¿å­˜çŠ¶æ€å’ŒMCTSçš„policy
            history.push((game.to_tensor(), policy.clone(), game.current_player()));
            
            // é€‰æ‹©æœ€ä½³åŠ¨ä½œ
            let action = mcts.select_action(0.5); // é€‚åº¦çš„æ¸©åº¦ä¿æŒæ¢ç´¢
            
            game.play(action).expect("éæ³•åŠ¨ä½œ");
        }
        
        // æ ¹æ®æ¸¸æˆç»“æœåˆ›å»ºè®­ç»ƒæ ·æœ¬
        let winner = game.winner();
        for (board, policy, player) in history {
            let outcome = match winner {
                Some(0) => 0.0,
                Some(w) if w == player => 1.0,
                Some(_) => -1.0,
                None => 0.0,
            };
            
            all_samples.push(TrainingSample {
                board,
                policy,
                outcome,
            });
        }
    }
    
    all_samples
}

/// è¯„ä¼°æ¨¡å‹å¯¹éšæœºç©å®¶çš„èƒœç‡
fn evaluate_model(trainer: &AlphaZeroTrainer, num_games: usize) -> f32 {
    let mut wins = 0;
    
    // æ³¨æ„ï¼šç”±äºMPSçš„float64é™åˆ¶ï¼Œè¯„ä¼°æ—¶ç®€å•åœ°æµ‹è¯•æ¨¡å‹
    // å®é™…ä¸Šæˆ‘ä»¬åªéœ€è¦å¿«é€Ÿæ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨å­¦ä¹ 
    for _ in 0..num_games {
        let mut game = Connect4::new();
        
        while !game.is_game_over() {
            if game.current_player() == 1 {
                // AIå›åˆï¼šä½¿ç”¨æ¨¡å‹é¢„æµ‹æœ€ä½³åŠ¨ä½œï¼ˆä¸ç”¨MCTSï¼Œé¿å…MPSé—®é¢˜ï¼‰
                let (policy, _) = trainer.predict(&game);
                
                // ç›´æ¥ä»policyä¸­é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ
                // ç¡¬ç¼–ç size=7é¿å…è°ƒç”¨policy.size()è§¦å‘MPSè½¬æ¢
                let legal_moves = game.legal_moves();
                let policy_vec: Vec<f32> = unsafe {
                    let data_ptr = policy.data_ptr() as *const f32;
                    std::slice::from_raw_parts(data_ptr, 7).to_vec()  // Connect4å›ºå®š7åˆ—
                };
                
                let mut best_action = legal_moves[0];
                let mut best_prob = -1.0f32;
                for &action in &legal_moves {
                    if policy_vec[action] > best_prob {
                        best_prob = policy_vec[action];
                        best_action = action;
                    }
                }
                
                game.play(best_action).ok();
            } else {
                // éšæœºç©å®¶
                let valid_moves = game.legal_moves();
                if !valid_moves.is_empty() {
                    use rand::Rng;
                    let random_move = valid_moves[rand::thread_rng().gen_range(0..valid_moves.len())];
                    game.play(random_move).ok();
                }
            }
        }
        
        if game.winner() == Some(1) {
            wins += 1;
        }
    }
    
    wins as f32 / num_games as f32
}

/// æ£€æŸ¥ç©ºæ£‹ç›˜çš„policyæ˜¯å¦æœ‰åå·®
fn check_policy_bias(trainer: &AlphaZeroTrainer) {
    let game = Connect4::new();
    let (policy, value) = trainer.predict(&game);
    
    println!("ğŸ” ç©ºæ£‹ç›˜policyæ£€æŸ¥:");
    println!("  Valueé¢„æµ‹: {:.4}", value);
    println!("  Policyåˆ†å¸ƒ:");
    
    // ç›´æ¥ä»policy tensorè¯»å–æ•°æ®ï¼Œé¿å…MPS float64é—®é¢˜
    // ç¡¬ç¼–ç size=7é¿å…è°ƒç”¨policy.size()
    let policy_vec: Vec<f32> = unsafe {
        let data_ptr = policy.data_ptr() as *const f32;
        std::slice::from_raw_parts(data_ptr, 7).to_vec()  // Connect4å›ºå®š7åˆ—
    };
    
    let max_prob = policy_vec.iter().cloned().fold(0.0f32, f32::max);
    
    for (col, &prob) in policy_vec.iter().enumerate() {
        let bar_length = (prob / max_prob * 50.0) as usize;
        let bar = "â–ˆ".repeat(bar_length);
        println!("    Col {}: {:.4} {}", col, prob, bar);
    }
    
    // æ£€æŸ¥æ˜¯å¦æœ‰æŸä¸€åˆ—è¿‡äºçªå‡º
    let max_col = policy_vec.iter()
        .enumerate()
        .max_by(|(_, a), (_, b): &(usize, &f32)| a.partial_cmp(b).unwrap())
        .map(|(idx, _)| idx)
        .unwrap_or(0);
    
    if max_prob > 0.3 {
        println!("  âš ï¸  æ£€æµ‹åˆ°åå·®ï¼šcolumn {} çš„æ¦‚ç‡è¿‡é«˜ ({:.1}%)", max_col, max_prob * 100.0);
    } else {
        println!("  âœ… åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€");
    }
}
