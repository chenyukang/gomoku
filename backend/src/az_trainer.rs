// AlphaZero è®­ç»ƒå™¨ - è‡ªå¯¹å¼ˆå’Œè®­ç»ƒé€»è¾‘

#![cfg(feature = "alphazero")]

use super::az_mcts_rollout::MCTSWithRollout;
use super::az_net::{Connect4Net, Connect4Trainer};
use super::connect4::Connect4;
use serde::{Deserialize, Serialize};
use std::collections::VecDeque;
use std::convert::TryFrom;
use tch::Tensor;

/// è®­ç»ƒæ ·æœ¬
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingSample {
    pub board: Vec<f32>,  // (3, 6, 7) flattenåçš„æ£‹ç›˜
    pub policy: Vec<f32>, // (7,) MCTSæœç´¢å¾—åˆ°çš„ç­–ç•¥
    pub outcome: f32,     // æ¸¸æˆç»“æœ: 1=å½“å‰ç©å®¶èµ¢, -1=è¾“, 0=å¹³å±€
}

/// å›æ”¾ç¼“å†²åŒº
pub struct ReplayBuffer {
    samples: VecDeque<TrainingSample>,
    max_size: usize,
}

impl ReplayBuffer {
    pub fn new(max_size: usize) -> Self {
        Self {
            samples: VecDeque::with_capacity(max_size),
            max_size,
        }
    }

    pub fn add(&mut self, sample: TrainingSample) {
        if self.samples.len() >= self.max_size {
            self.samples.pop_front();
        }
        self.samples.push_back(sample);
    }

    pub fn add_batch(&mut self, samples: Vec<TrainingSample>) {
        for sample in samples {
            self.add(sample);
        }
    }

    pub fn sample_batch(&self, batch_size: usize) -> Option<Vec<TrainingSample>> {
        if self.samples.len() < batch_size {
            return None;
        }

        use rand::seq::SliceRandom;
        let mut rng = rand::thread_rng();
        let samples_vec: Vec<_> = self.samples.iter().cloned().collect();
        Some(
            samples_vec
                .choose_multiple(&mut rng, batch_size)
                .cloned()
                .collect(),
        )
    }

    pub fn len(&self) -> usize {
        self.samples.len()
    }
}

pub struct AlphaZeroTrainer {
    pub trainer: Connect4Trainer,
    replay_buffer: ReplayBuffer,
    num_mcts_simulations: u32,
}

impl AlphaZeroTrainer {
    pub fn new(
        num_filters: i64,
        learning_rate: f64,
        replay_buffer_size: usize,
        num_mcts_simulations: u32,
    ) -> Self {
        Self {
            trainer: Connect4Trainer::new(num_filters, learning_rate),
            replay_buffer: ReplayBuffer::new(replay_buffer_size),
            num_mcts_simulations,
        }
    }

    /// è¿›è¡Œä¸€å±€è‡ªå¯¹å¼ˆ
    pub fn self_play_game(&mut self, temperature: f32) -> Vec<TrainingSample> {
        let mut game = Connect4::new();
        let mut history = Vec::new();

        // æ¸¸æˆå¾ªç¯
        while !game.is_game_over() {
            // ä½¿ç”¨MCTSæœç´¢ - å…³é”®æ”¹è¿›ï¼šä½¿ç”¨rolloutè€Œéç½‘ç»œvalue
            let mut mcts = MCTSWithRollout::new(self.num_mcts_simulations, true);
            let policy = mcts.search(&game, &self.trainer.net);

            // ä¿å­˜å½“å‰çŠ¶æ€
            history.push((game.to_tensor(), policy.clone(), game.current_player()));

            // é€‰æ‹©åŠ¨ä½œï¼ˆä½¿ç”¨æ¸©åº¦ï¼‰
            let action = mcts.select_action(temperature);

            // æ‰§è¡ŒåŠ¨ä½œ
            game.play(action).expect("éæ³•åŠ¨ä½œ");
        }

        // æ ¹æ®æ¸¸æˆç»“æœç”Ÿæˆè®­ç»ƒæ ·æœ¬
        self.create_training_samples(history, game.winner())
    }

    /// æ ¹æ®æ¸¸æˆç»“æœåˆ›å»ºè®­ç»ƒæ ·æœ¬
    fn create_training_samples(
        &self,
        history: Vec<(Vec<f32>, Vec<f32>, u8)>,
        winner: Option<u8>,
    ) -> Vec<TrainingSample> {
        let mut samples = Vec::new();

        for (board, policy, player) in history {
            // è®¡ç®—è¯¥æ ·æœ¬çš„ç»“æœ
            let outcome = match winner {
                Some(0) => 0.0,                // å¹³å±€
                Some(w) if w == player => 1.0, // è¯¥ç©å®¶èµ¢äº†
                Some(_) => -1.0,               // è¯¥ç©å®¶è¾“äº†
                None => 0.0,                   // ä¸åº”è¯¥å‘ç”Ÿ
            };

            samples.push(TrainingSample {
                board,
                policy,
                outcome,
            });
        }

        samples
    }

    /// ç”Ÿæˆå¤šå±€è‡ªå¯¹å¼ˆæ•°æ®
    pub fn generate_self_play_data(&mut self, num_games: usize, temperature: f32) {
        println!("ğŸ® ç”Ÿæˆ {} å±€è‡ªå¯¹å¼ˆæ•°æ®...", num_games);

        for i in 0..num_games {
            let samples = self.self_play_game(temperature);
            self.replay_buffer.add_batch(samples);

            if (i + 1) % 10 == 0 {
                println!("  å®Œæˆ {}/{} å±€", i + 1, num_games);
            }
        }

        println!("âœ… è‡ªå¯¹å¼ˆå®Œæˆï¼Œç¼“å†²åŒºå¤§å°: {}", self.replay_buffer.len());
    }

    /// è®­ç»ƒç½‘ç»œ
    pub fn train(&mut self, batch_size: usize, num_iterations: usize) {
        println!(
            "ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œæ‰¹æ¬¡å¤§å°: {}, è¿­ä»£æ¬¡æ•°: {}",
            batch_size, num_iterations
        );

        let mut total_loss = 0.0;
        let mut total_policy_loss = 0.0;
        let mut total_value_loss = 0.0;

        for i in 0..num_iterations {
            if let Some(batch) = self.replay_buffer.sample_batch(batch_size) {
                let (boards, policies, values) = self.prepare_batch(batch);
                let (loss, policy_loss, value_loss) =
                    self.trainer.train_batch(&boards, &policies, &values);

                total_loss += loss;
                total_policy_loss += policy_loss;
                total_value_loss += value_loss;

                if (i + 1) % 20 == 0 {
                    println!(
                        "  è¿­ä»£ {}/{}: loss={:.4} (policy={:.4}, value={:.4})",
                        i + 1,
                        num_iterations,
                        loss,
                        policy_loss,
                        value_loss
                    );
                }
            } else {
                println!("âš ï¸  ç¼“å†²åŒºæ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡è®­ç»ƒ");
                break;
            }
        }

        let avg_loss = total_loss / num_iterations as f64;
        let avg_policy_loss = total_policy_loss / num_iterations as f64;
        let avg_value_loss = total_value_loss / num_iterations as f64;

        println!(
            "âœ… è®­ç»ƒå®Œæˆï¼Œå¹³å‡æŸå¤±: {:.4} (policy={:.4}, value={:.4})",
            avg_loss, avg_policy_loss, avg_value_loss
        );
    }

    /// å‡†å¤‡è®­ç»ƒæ‰¹æ¬¡
    fn prepare_batch(&self, batch: Vec<TrainingSample>) -> (Tensor, Tensor, Tensor) {
        let batch_size = batch.len();

        // æå–æ•°æ®
        let boards: Vec<f32> = batch.iter().flat_map(|s| s.board.iter().cloned()).collect();

        let policies: Vec<f32> = batch
            .iter()
            .flat_map(|s| s.policy.iter().cloned())
            .collect();

        let values: Vec<f32> = batch.iter().map(|s| s.outcome).collect();

        // è½¬æ¢ä¸ºå¼ é‡
        let boards_tensor =
            Tensor::f_from_slice(&boards)
                .unwrap()
                .reshape(&[batch_size as i64, 3, 6, 7]);

        let policies_tensor = Tensor::f_from_slice(&policies)
            .unwrap()
            .reshape(&[batch_size as i64, 7]);

        let values_tensor = Tensor::f_from_slice(&values)
            .unwrap()
            .reshape(&[batch_size as i64, 1]);

        (boards_tensor, policies_tensor, values_tensor)
    }

    /// ä¿å­˜æ¨¡å‹
    pub fn save_model(&mut self, path: &str) -> Result<(), Box<dyn std::error::Error>> {
        self.trainer.save(path)
    }

    /// åŠ è½½æ¨¡å‹
    pub fn load_model(&mut self, path: &str) -> Result<(), Box<dyn std::error::Error>> {
        self.trainer.load(path)
    }

    /// ç›´æ¥æ·»åŠ è®­ç»ƒæ ·æœ¬åˆ°replay buffer
    pub fn add_sample(&mut self, sample: TrainingSample) {
        self.replay_buffer.add(sample);
    }

    /// è·å–replay bufferå¤§å°
    pub fn replay_buffer_size(&self) -> usize {
        self.replay_buffer.len()
    }

    /// è·å–ç½‘ç»œå¼•ç”¨ï¼ˆç”¨äºMCTSï¼‰
    pub fn get_net(&self) -> &Connect4Net {
        &self.trainer.net
    }

    /// ä½¿ç”¨ç½‘ç»œé¢„æµ‹ä¸€ä¸ªå±€é¢
    pub fn predict(&self, game: &Connect4) -> (Tensor, f32) {
        let device = self.trainer.device();
        let board_tensor = Tensor::from_slice(&game.to_tensor())
            .view([1, 3, 6, 7])
            .to_device(device);
        
        let (policy_logits, value) = self.trainer.net.forward(&board_tensor, false);
        
        // è·å–åˆæ³•åŠ¨ä½œ
        let valid_moves = game.legal_moves();
        let mut mask = vec![-1e9; 7];
        for &m in &valid_moves {
            mask[m] = 0.0;
        }
        let mask_tensor = Tensor::from_slice(&mask)
            .view([1, 7])
            .to_device(device);
        
        // åº”ç”¨maskå¹¶softmax
        let policy = (policy_logits + mask_tensor).softmax(1, tch::Kind::Float);
        
        // ç›´æ¥ä»value tensorè·å–æ ‡é‡å€¼ï¼Œé¿å…MPS float64é—®é¢˜
        // ä½¿ç”¨å†…éƒ¨æ–¹æ³•ç›´æ¥è¯»å–ï¼Œä¸ç»è¿‡ç±»å‹è½¬æ¢
        let value_scalar = unsafe {
            let value_data = value.data_ptr() as *const f32;
            *value_data
        };
        
        (policy.squeeze(), value_scalar)
    }
}
