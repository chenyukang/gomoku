// AlphaZero ç¥ç»ç½‘ç»œ
// ä½¿ç”¨ tch-rs (PyTorch for Rust)

#![cfg(feature = "alphazero")]

use tch::{nn, nn::OptimizerConfig, Device, Tensor};

/// æ®‹å·®å—
#[derive(Debug)]
struct ResidualBlock {
    conv1: nn::Conv2D,
    bn1: nn::BatchNorm,
    conv2: nn::Conv2D,
    bn2: nn::BatchNorm,
}

impl ResidualBlock {
    fn new(vs: &nn::Path, channels: i64) -> Self {
        let conv_config = nn::ConvConfig {
            padding: 1,
            ..Default::default()
        };

        Self {
            conv1: nn::conv2d(vs, channels, channels, 3, conv_config),
            bn1: nn::batch_norm2d(vs, channels, Default::default()),
            conv2: nn::conv2d(vs, channels, channels, 3, conv_config),
            bn2: nn::batch_norm2d(vs, channels, Default::default()),
        }
    }

    fn forward(&self, xs: &Tensor, train: bool) -> Tensor {
        let residual = xs.shallow_clone();

        let out = xs
            .apply(&self.conv1)
            .apply_t(&self.bn1, train)
            .relu()
            .apply(&self.conv2)
            .apply_t(&self.bn2, train);

        (out + residual).relu()
    }
}

/// AlphaZero åŒå¤´ç½‘ç»œï¼ˆç­–ç•¥ + ä»·å€¼ï¼‰
#[derive(Debug)]
pub struct AlphaZeroNet {
    // å…±äº«ç‰¹å¾æå–å±‚
    conv_input: nn::Conv2D,
    bn_input: nn::BatchNorm,
    res_blocks: Vec<ResidualBlock>,

    // ç­–ç•¥å¤´ï¼ˆPolicy Headï¼‰
    policy_conv: nn::Conv2D,
    policy_bn: nn::BatchNorm,
    policy_fc: nn::Linear,

    // ä»·å€¼å¤´ï¼ˆValue Headï¼‰
    value_conv: nn::Conv2D,
    value_bn: nn::BatchNorm,
    value_fc1: nn::Linear,
    value_fc2: nn::Linear,

    num_filters: i64,
    device: Device,
}

impl AlphaZeroNet {
    /// åˆ›å»ºæ–°çš„ AlphaZero ç½‘ç»œ
    ///
    /// # å‚æ•°
    /// - num_filters: å·ç§¯å±‚çš„æ»¤æ³¢å™¨æ•°é‡ï¼ˆé»˜è®¤ 256ï¼‰
    /// - num_res_blocks: æ®‹å·®å—æ•°é‡ï¼ˆé»˜è®¤ 10-20ï¼‰
    pub fn new(vs: &nn::Path, num_filters: i64, num_res_blocks: i64) -> Self {
        let device = vs.device();

        // è¾“å…¥å±‚ï¼šå°†æ£‹ç›˜è½¬æ¢ä¸ºç‰¹å¾
        let conv_config = nn::ConvConfig {
            padding: 1,
            ..Default::default()
        };
        let conv_input = nn::conv2d(vs / "conv_input", 3, num_filters, 3, conv_config);
        let bn_input = nn::batch_norm2d(vs / "bn_input", num_filters, Default::default());

        // æ®‹å·®å—
        let mut res_blocks = Vec::new();
        for i in 0..num_res_blocks {
            let block = ResidualBlock::new(&(vs / format!("res_block_{}", i)), num_filters);
            res_blocks.push(block);
        }

        // ç­–ç•¥å¤´
        let policy_conv = nn::conv2d(vs / "policy_conv", num_filters, 2, 1, Default::default());
        let policy_bn = nn::batch_norm2d(vs / "policy_bn", 2, Default::default());
        let policy_fc = nn::linear(vs / "policy_fc", 2 * 15 * 15, 15 * 15, Default::default());

        // ä»·å€¼å¤´
        let value_conv = nn::conv2d(vs / "value_conv", num_filters, 1, 1, Default::default());
        let value_bn = nn::batch_norm2d(vs / "value_bn", 1, Default::default());
        let value_fc1 = nn::linear(vs / "value_fc1", 15 * 15, 256, Default::default());
        let value_fc2 = nn::linear(vs / "value_fc2", 256, 1, Default::default());

        Self {
            conv_input,
            bn_input,
            res_blocks,
            policy_conv,
            policy_bn,
            policy_fc,
            value_conv,
            value_bn,
            value_fc1,
            value_fc2,
            num_filters,
            device,
        }
    }

    /// è·å–æ»¤æ³¢å™¨æ•°é‡
    pub fn num_filters(&self) -> i64 {
        self.num_filters
    }

    /// è·å–æ®‹å·®å—æ•°é‡
    pub fn num_res_blocks(&self) -> i64 {
        self.res_blocks.len() as i64
    }

    /// è·å–è®¾å¤‡
    pub fn device(&self) -> Device {
        self.device
    }

    /// å‰å‘ä¼ æ’­
    ///
    /// # è¾“å…¥
    /// - xs: (batch, 3, 15, 15) çš„æ£‹ç›˜çŠ¶æ€
    ///   - Channel 0: å½“å‰ç©å®¶çš„æ£‹å­
    ///   - Channel 1: å¯¹æ‰‹çš„æ£‹å­
    ///   - Channel 2: å½“å‰ç©å®¶æ ‡è®°ï¼ˆå…¨1æˆ–å…¨0ï¼‰
    ///
    /// # è¾“å‡º
    /// - policy: (batch, 225) æ¯ä¸ªä½ç½®çš„æ¦‚ç‡
    /// - value: (batch, 1) å±€é¢è¯„ä¼°å€¼ [-1, 1]
    pub fn forward(&self, xs: &Tensor, train: bool) -> (Tensor, Tensor) {
        // å…±äº«ç‰¹å¾æå–
        let mut out = xs
            .apply(&self.conv_input)
            .apply_t(&self.bn_input, train)
            .relu();

        // æ®‹å·®å—
        for block in &self.res_blocks {
            out = block.forward(&out, train);
        }

        // ç­–ç•¥å¤´
        let policy = out
            .apply(&self.policy_conv)
            .apply_t(&self.policy_bn, train)
            .relu()
            .flatten(1, -1)
            .apply(&self.policy_fc)
            .log_softmax(-1, tch::Kind::Float);

        // ä»·å€¼å¤´
        let value = out
            .apply(&self.value_conv)
            .apply_t(&self.value_bn, train)
            .relu()
            .flatten(1, -1)
            .apply(&self.value_fc1)
            .relu()
            .apply(&self.value_fc2)
            .tanh();

        (policy, value)
    }

    /// é¢„æµ‹ï¼ˆä¸è®­ç»ƒæ¨¡å¼ï¼‰
    pub fn predict(&self, board: &Tensor) -> (Tensor, Tensor) {
        tch::no_grad(|| self.forward(board, false))
    }

    /// ä¿å­˜æ¨¡å‹
    pub fn save(
        &mut self,
        vs: &mut nn::VarStore,
        path: &str,
    ) -> Result<(), Box<dyn std::error::Error>> {
        vs.save(path)?;
        println!("âœ… Model saved to {}", path);
        Ok(())
    }

    /// åŠ è½½æ¨¡å‹
    pub fn load(vs: &mut nn::VarStore, path: &str) -> Result<(), Box<dyn std::error::Error>> {
        vs.load(path)?;
        println!("âœ… Model loaded from {}", path);
        Ok(())
    }
}

/// AlphaZero è®­ç»ƒå™¨
pub struct AlphaZeroTrainer {
    vs: nn::VarStore,
    net: AlphaZeroNet,
    optimizer: nn::Optimizer,
}

impl AlphaZeroTrainer {
    pub fn new(num_filters: i64, num_res_blocks: i64, learning_rate: f64) -> Self {
        let vs = nn::VarStore::new(Device::cuda_if_available());
        let net = AlphaZeroNet::new(&vs.root(), num_filters, num_res_blocks);
        let optimizer = nn::Adam::default().build(&vs, learning_rate).unwrap();

        Self { vs, net, optimizer }
    }

    /// è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡
    ///
    /// # è¾“å…¥
    /// - boards: (batch, 3, 15, 15) æ£‹ç›˜çŠ¶æ€
    /// - policy_targets: (batch, 225) ç›®æ ‡ç­–ç•¥ï¼ˆæ¥è‡ª MCTSï¼‰
    /// - value_targets: (batch, 1) ç›®æ ‡ä»·å€¼ï¼ˆæ¸¸æˆç»“æœï¼‰
    pub fn train_batch(
        &mut self,
        boards: &Tensor,
        policy_targets: &Tensor,
        value_targets: &Tensor,
    ) -> (f64, f64, f64) {
        // å‰å‘ä¼ æ’­
        let (policy_pred, value_pred) = self.net.forward(boards, true);

        // ç­–ç•¥æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
        let policy_loss = -(policy_targets * &policy_pred)
            .sum(tch::Kind::Float)
            .mean(tch::Kind::Float);

        // ä»·å€¼æŸå¤±ï¼ˆMSEï¼‰
        let value_loss = (&value_pred - value_targets)
            .pow_tensor_scalar(2)
            .mean(tch::Kind::Float);

        // æ€»æŸå¤±
        let total_loss = &policy_loss + &value_loss;

        // åå‘ä¼ æ’­
        self.optimizer.backward_step(&total_loss);

        (
            total_loss.double_value(&[]),
            policy_loss.double_value(&[]),
            value_loss.double_value(&[]),
        )
    }

    /// è·å–ç½‘ç»œå¼•ç”¨
    pub fn net(&self) -> &AlphaZeroNet {
        &self.net
    }

    /// ä¿å­˜æ¨¡å‹
    pub fn save(&mut self, path: &str) -> Result<(), Box<dyn std::error::Error>> {
        self.net.save(&mut self.vs, path)
    }

    /// åŠ è½½æ¨¡å‹
    pub fn load(&mut self, path: &str) -> Result<(), Box<dyn std::error::Error>> {
        AlphaZeroNet::load(&mut self.vs, path)
    }

    /// ä»æ–‡ä»¶åˆ›å»ºè®­ç»ƒå™¨ï¼ˆç”¨äºå¹¶è¡Œè®­ç»ƒï¼‰
    /// å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ¨¡å‹
    pub fn from_file(
        path: &str,
        learning_rate: f64,
        num_filters: i64,
        num_res_blocks: i64,
    ) -> Self {
        let device = Device::cuda_if_available();
        let mut vs = nn::VarStore::new(device);
        let net = AlphaZeroNet::new(&vs.root(), num_filters, num_res_blocks);

        // å°è¯•åŠ è½½æ¨¡å‹å‚æ•°
        if std::path::Path::new(path).exists() {
            match vs.load(path) {
                Ok(_) => println!("âœ… Loaded existing model from {}", path),
                Err(e) => {
                    eprintln!("âš ï¸  Failed to load model ({}), using new model instead", e);
                }
            }
        } else {
            println!("ğŸ“ Creating new model (no checkpoint found at {})", path);
        }

        let optimizer = nn::Adam::default().build(&vs, learning_rate).unwrap();

        Self { vs, net, optimizer }
    }

    /// åœ¨ç»™å®šæ ·æœ¬ä¸Šè®­ç»ƒï¼ˆç”¨äºå¹¶è¡Œè®­ç»ƒï¼‰
    pub fn train_on_samples(
        &mut self,
        samples: &[super::alphazero_trainer::TrainingSample],
        num_iterations: usize,
    ) {
        use rand::seq::SliceRandom;
        use rand::thread_rng;

        let batch_size = 32;
        let mut rng = thread_rng();

        println!(
            "Training on {} samples for {} iterations",
            samples.len(),
            num_iterations
        );

        for iter in 0..num_iterations {
            // éšæœºé‡‡æ ·ä¸€ä¸ªæ‰¹æ¬¡
            let batch: Vec<_> = samples
                .choose_multiple(&mut rng, batch_size.min(samples.len()))
                .cloned()
                .collect();

            if batch.is_empty() {
                continue;
            }

            // å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            let boards: Vec<f32> = batch.iter().flat_map(|s| s.board.iter().copied()).collect();
            let policies: Vec<f32> = batch
                .iter()
                .flat_map(|s| s.policy.iter().copied())
                .collect();
            let values: Vec<f32> = batch.iter().map(|s| s.value).collect();

            let boards_tensor = Tensor::f_from_slice(&boards)
                .unwrap()
                .reshape(&[batch.len() as i64, 3, 15, 15])
                .to_device(self.vs.device());

            let policies_tensor = Tensor::f_from_slice(&policies)
                .unwrap()
                .reshape(&[batch.len() as i64, 225])
                .to_device(self.vs.device());

            let values_tensor = Tensor::f_from_slice(&values)
                .unwrap()
                .reshape(&[batch.len() as i64, 1])
                .to_device(self.vs.device());

            // è®­ç»ƒ
            let (total_loss, policy_loss, value_loss) =
                self.train_batch(&boards_tensor, &policies_tensor, &values_tensor);

            if (iter + 1) % 50 == 0 || iter == 0 {
                println!(
                    "  Iteration {}/{}: loss={:.4} (policy={:.4}, value={:.4})",
                    iter + 1,
                    num_iterations,
                    total_loss,
                    policy_loss,
                    value_loss
                );
            }
        }
    }

    /// ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶
    pub fn save_model(&mut self, path: &str) -> Result<(), Box<dyn std::error::Error>> {
        self.save(path)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    #[cfg(feature = "alphazero")]
    fn test_network_creation() {
        let vs = nn::VarStore::new(Device::Cpu);
        let net = AlphaZeroNet::new(&vs.root(), 64, 5);

        // åˆ›å»ºä¸€ä¸ªéšæœºè¾“å…¥
        let input = Tensor::randn(&[1, 3, 15, 15], (tch::Kind::Float, Device::Cpu));
        let (policy, value) = net.predict(&input);

        assert_eq!(policy.size(), vec![1, 225]);
        assert_eq!(value.size(), vec![1, 1]);

        println!("âœ… Network test passed");
    }
}
