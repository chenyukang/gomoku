# AlphaZero 训练问题诊断与解决方案

## 🐛 观察到的问题

从训练日志中发现：
- **初始**: vs随机 30%, vs MCTS 0%
- **迭代3-14**: vs随机 0-20%波动, vs MCTS 持续0%
- **Loss**: 在1.5-1.8之间震荡，没有持续下降

**结论**: 模型没有学到有效策略，甚至越训练越差！

## 🔍 根本原因分析

### 1. **MCTS模拟次数太少** ⚠️⚠️⚠️ (最关键)

**当前设置**: 50次模拟
**问题**: 当神经网络质量很差时，50次模拟产生的策略也很差
- 初期网络输出接近随机 → MCTS得到的策略质量低
- 用低质量数据训练 → 网络变得更差
- 形成**恶性循环**: 差网络→差数据→更差网络

**证据**:
- 纯MCTS(50模拟) vs 随机玩家 = 100%胜率
- AlphaZero(50模拟+差网络) vs 随机 = 0-30%
- 说明网络引导反而让MCTS变弱了！

**解决方案**: 增加到200-400次模拟
- 更多模拟 → 即使网络差，MCTS也能靠搜索找到好策略
- 产生高质量训练数据 → 网络能学到正确信号

### 2. **温度参数使用不当**

**当前设置**: 全程 temperature=1.0
**问题**:
- 高温度 = 高随机性，即使找到好走法也可能不选
- 训练数据中充满次优走法
- 网络学到的是"平均策略"而非"最优策略"

**AlphaGo/AlphaZero做法**:
```
前期(1-30步): temp=1.0  (高探索)
后期(30+步): temp=0.01  (接近确定性)
```

### 3. **网络容量不足**

**当前**: 32 filters
**问题**: Connect4虽然简单，但32 filters可能不够
**建议**: 64-128 filters

### 4. **学习率太高**

**当前**: 0.001
**问题**: 学习率高 → 参数大幅震荡 → loss不收敛
**建议**: 0.0003 或更小

### 5. **训练数据量不足**

**当前**: 每轮20局 × 30步 ≈ 600样本
**问题**: 样本太少，网络容易过拟合到当前策略
**建议**: 每轮30-50局

## ✅ 改进方案

### 核心改进（按重要性排序）

| 参数 | 原值 | 新值 | 重要性 | 原因 |
|------|------|------|--------|------|
| **MCTS模拟次数** | 50 | 200-400 | ⭐⭐⭐⭐⭐ | 最关键！保证数据质量 |
| **温度策略** | 固定1.0 | 前期1.0→后期0.1 | ⭐⭐⭐⭐ | 学习最优策略 |
| **网络容量** | 32 | 64-128 | ⭐⭐⭐ | 提升表达能力 |
| **学习率** | 0.001 | 0.0003 | ⭐⭐⭐ | 稳定训练 |
| **每轮游戏数** | 20 | 30-50 | ⭐⭐ | 增加数据多样性 |

### 代码实现

见 `train_improved.rs`:
```rust
let num_mcts_simulations = 200;  // 关键改进！
let num_filters = 64;
let learning_rate = 0.0003;
let games_per_iteration = 30;

// 动态温度
let temperature = if iteration < 5 {
    1.0   // 前期高探索
} else if iteration < 15 {
    0.5   // 中期
} else {
    0.1   // 后期确定性
};
```

## 📊 预期改进效果

**改进前** (当前):
- 迭代3: vs随机 20%, vs MCTS 0%
- 迭代10: vs随机 20%, vs MCTS 0%
- Loss震荡在1.5-1.8

**改进后** (预期):
- 迭代3: vs随机 40-60%, vs MCTS 10-20%
- 迭代10: vs随机 70-90%, vs MCTS 30-50%
- 迭代30: vs随机 90%+, vs MCTS 60%+
- Loss持续下降: 2.0 → 1.5 → 1.0 → 0.7

## 🚀 运行改进版本

```bash
cd backend
cargo run --features alphazero --bin train_improved
```

**注意**:
- 训练会变慢（MCTS 50→200次，慢4倍）
- 但数据质量大幅提升，训练会真正收敛
- 预计需要2-3小时完成30轮

## 📚 参考: AlphaZero原始论文的做法

DeepMind的AlphaGo Zero论文中:
- **MCTS模拟**: 1600次/步 (围棋)
- **温度**: 前30步=1，之后=0.001
- **网络**: 20个残差块 (远大于我们的3层卷积)
- **自对弈**: 每轮25000局
- **训练时间**: 数天到数周

我们的Connect4简单得多，所以:
- MCTS 200次就够了（不需要1600）
- 网络64 filters足够（不需要20个残差块）
- 每轮30局足够（不需要25000）
- 但核心原理一样：**高质量自对弈数据是关键**

## 🎯 调试技巧

### 如何判断训练是否正常

**好的训练曲线**:
```
vs随机: 30% → 50% → 70% → 90%+ (稳步上升)
vs MCTS: 0% → 10% → 30% → 50%+ (逐步提升)
Loss: 2.0 → 1.5 → 1.0 → 0.7 (持续下降)
```

**坏的训练曲线** (当前问题):
```
vs随机: 30% → 20% → 0% → 10% (下降或震荡)
vs MCTS: 0% → 0% → 0% → 0% (无改进)
Loss: 1.8 → 1.6 → 1.7 → 1.5 (震荡不收敛)
```

### 快速验证改进是否有效

运行3轮即可看出趋势:
```bash
# 只跑3轮快速测试
# 修改 train_improved.rs: let num_iterations = 3;
cargo run --features alphazero --bin train_improved
```

如果3轮后：
- vs随机 > 40%: ✅ 有希望
- vs随机 < 30%: ❌ 还有问题
- Loss明显下降: ✅ 正常
- Loss震荡: ❌ 学习率还太高

## 💡 进一步优化方向

如果改进版本效果好，可以考虑：

1. **残差网络**: 用ResNet替换普通卷积
2. **数据增强**: 利用棋盘对称性（翻转、镜像）
3. **优先级回放**: 重要样本多训练
4. **对抗训练**: 和历史最佳模型对战
5. **分布式自对弈**: 并行生成数据

但现在最重要的是：**先让基础版本能work**！
