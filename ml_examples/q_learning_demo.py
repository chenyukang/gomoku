#!/usr/bin/env python3
"""
ç®€å•çš„ Q-Learning å®ç°ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªæ•™å­¦ç”¨çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå±•ç¤ºå¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µã€‚
åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„çŠ¶æ€è¡¨ç¤ºå’Œç½‘ç»œæ¶æ„ã€‚
"""

import numpy as np
import pickle
from collections import defaultdict

class SimpleQLearning:
    """
    ç®€å•çš„ Q-Learning ç®—æ³•å®ç°

    çŠ¶æ€: ç®€åŒ–çš„æ£‹ç›˜è¡¨ç¤º (å¯ä»¥æ”¹è¿›ä¸ºæ›´å¤æ‚çš„ç‰¹å¾)
    åŠ¨ä½œ: æ£‹ç›˜ä¸Šçš„ä½ç½® (x, y)
    å¥–åŠ±: èµ¢ = +1, è¾“ = -1, å¹³å±€ = 0, ä¸­é—´æ­¥éª¤ = -0.01
    """

    def __init__(self,
                 learning_rate=0.1,
                 discount_factor=0.95,
                 epsilon=0.1):
        """
        å‚æ•°:
            learning_rate: å­¦ä¹ ç‡ (alpha)
            discount_factor: æŠ˜æ‰£å› å­ (gamma)
            epsilon: æ¢ç´¢ç‡ (epsilon-greedy)
        """
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon

        # Q-table: (state, action) -> Q-value
        # ä½¿ç”¨ defaultdict è‡ªåŠ¨åˆå§‹åŒ–ä¸º 0
        self.q_table = defaultdict(float)

        # ç»Ÿè®¡
        self.total_updates = 0

    def state_to_key(self, board_str):
        """
        å°†æ£‹ç›˜çŠ¶æ€è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„ key

        å®é™…åº”ç”¨ä¸­å¯ä»¥ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•:
        - å¯¹ç§°æ€§å¤„ç† (æ—‹è½¬ã€é•œåƒ)
        - å±€éƒ¨ç‰¹å¾æå–
        - ç¥ç»ç½‘ç»œç¼–ç 
        """
        return board_str

    def get_q_value(self, state, action):
        """è·å– Q å€¼"""
        key = (self.state_to_key(state), action)
        return self.q_table[key]

    def update_q_value(self, state, action, reward, next_state, next_valid_actions):
        """
        æ›´æ–° Q å€¼

        Q(s,a) <- Q(s,a) + Î± * [r + Î³ * max_a' Q(s',a') - Q(s,a)]
        """
        current_q = self.get_q_value(state, action)

        # è®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æœ€å¤§ Q å€¼
        if next_valid_actions:
            max_next_q = max(self.get_q_value(next_state, a)
                            for a in next_valid_actions)
        else:
            max_next_q = 0  # ç»ˆæ­¢çŠ¶æ€

        # Q-learning æ›´æ–°å…¬å¼
        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)

        key = (self.state_to_key(state), action)
        self.q_table[key] = new_q
        self.total_updates += 1

    def choose_action(self, state, valid_actions):
        """
        é€‰æ‹©åŠ¨ä½œ (epsilon-greedy ç­–ç•¥)

        ä»¥ epsilon æ¦‚ç‡éšæœºæ¢ç´¢
        ä»¥ 1-epsilon æ¦‚ç‡é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        """
        if not valid_actions:
            return None

        # æ¢ç´¢
        if np.random.random() < self.epsilon:
            return valid_actions[np.random.randint(len(valid_actions))]

        # åˆ©ç”¨: é€‰æ‹© Q å€¼æœ€å¤§çš„åŠ¨ä½œ
        q_values = [(action, self.get_q_value(state, action))
                    for action in valid_actions]

        # æ‰¾å‡ºæœ€å¤§ Q å€¼
        max_q = max(q for _, q in q_values)

        # å¦‚æœæœ‰å¤šä¸ªæœ€å¤§å€¼ï¼Œéšæœºé€‰ä¸€ä¸ª
        best_actions = [action for action, q in q_values if q == max_q]
        return best_actions[np.random.randint(len(best_actions))]

    def save(self, filename):
        """ä¿å­˜ Q-table"""
        with open(filename, 'wb') as f:
            pickle.dump(dict(self.q_table), f)
        print(f"âœ… Q-table å·²ä¿å­˜åˆ° {filename}")
        print(f"   æ€»å…± {len(self.q_table)} ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹")

    def load(self, filename):
        """åŠ è½½ Q-table"""
        try:
            with open(filename, 'rb') as f:
                self.q_table = defaultdict(float, pickle.load(f))
            print(f"âœ… ä» {filename} åŠ è½½äº† {len(self.q_table)} ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹")
        except FileNotFoundError:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {filename}")

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'q_table_size': len(self.q_table),
            'total_updates': self.total_updates,
            'avg_q_value': np.mean(list(self.q_table.values())) if self.q_table else 0,
            'max_q_value': max(self.q_table.values()) if self.q_table else 0,
            'min_q_value': min(self.q_table.values()) if self.q_table else 0,
        }


def demo_training():
    """æ¼”ç¤ºè®­ç»ƒè¿‡ç¨‹ (ç®€åŒ–ç‰ˆ)"""
    print("="*60)
    print("ğŸ¯ Q-Learning æ¼”ç¤º")
    print("="*60)

    # åˆ›å»º Q-learning æ™ºèƒ½ä½“
    agent = SimpleQLearning(
        learning_rate=0.1,
        discount_factor=0.95,
        epsilon=0.2  # 20% æ¢ç´¢
    )

    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    print("\nğŸ“š è®­ç»ƒä¸­...")
    print("(è¿™åªæ˜¯ä¸€ä¸ªæ¼”ç¤ºï¼Œå®é™…éœ€è¦ä¸æ¸¸æˆç¯å¢ƒäº¤äº’)\n")

    # æ¨¡æ‹Ÿä¸€äº›çŠ¶æ€å’ŒåŠ¨ä½œ
    dummy_state = "." * 225  # ç©ºæ£‹ç›˜
    dummy_actions = [(7, 7), (7, 8), (8, 7), (8, 8)]  # ä¸­å¿ƒåŒºåŸŸ

    for episode in range(100):
        state = dummy_state
        action = agent.choose_action(state, dummy_actions)

        # æ¨¡æ‹Ÿå¥–åŠ± (å®é™…åº”è¯¥æ¥è‡ªæ¸¸æˆç»“æœ)
        reward = np.random.choice([1, -1, -0.01], p=[0.3, 0.3, 0.4])

        # æ¨¡æ‹Ÿä¸‹ä¸€ä¸ªçŠ¶æ€
        next_state = dummy_state  # ç®€åŒ–
        next_actions = dummy_actions

        # æ›´æ–° Q å€¼
        agent.update_q_value(state, action, reward, next_state, next_actions)

        if (episode + 1) % 20 == 0:
            print(f"Episode {episode + 1}/100 å®Œæˆ")

    # æ˜¾ç¤ºç»Ÿè®¡
    print("\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
    stats = agent.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")

    # ä¿å­˜æ¨¡å‹
    agent.save('data/q_table.pkl')

    print("\nğŸ’¡ å®é™…åº”ç”¨æ­¥éª¤:")
    print("1. ä¿®æ”¹ Rust ä»£ç ,æ·»åŠ  Q-learning æ™ºèƒ½ä½“")
    print("2. è®©æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’(è‡ªæˆ‘å¯¹å¼ˆ)")
    print("3. æ”¶é›†ç»éªŒå¹¶æ›´æ–° Q-table")
    print("4. è¯„ä¼°æ€§èƒ½å¹¶è°ƒæ•´è¶…å‚æ•°")


def integration_guide():
    """é›†æˆåˆ° Rust é¡¹ç›®çš„æŒ‡å—"""
    print("\n" + "="*60)
    print("ğŸ”§ ä¸ Rust é¡¹ç›®é›†æˆ")
    print("="*60)

    guide = """
æ–¹æ¡ˆ 1: Python ä½œä¸ºè®­ç»ƒå™¨
------------------------
1. Rust æä¾›æ¸¸æˆç¯å¢ƒå’Œå¿«é€Ÿæ¨¡æ‹Ÿ
2. Python è®­ç»ƒ Q-learning / ç¥ç»ç½‘ç»œ
3. å°†å­¦åˆ°çš„å‚æ•°å¯¼å‡ºä¸º JSON/äºŒè¿›åˆ¶
4. Rust åŠ è½½å‚æ•°ç”¨äºæ¨ç†

æ–¹æ¡ˆ 2: PyO3 é›†æˆ
-----------------
1. ä½¿ç”¨ PyO3 åœ¨ Rust ä¸­è°ƒç”¨ Python
2. è®­ç»ƒå’Œæ¨ç†éƒ½å¯ä»¥åœ¨ Rust ä¸­å®Œæˆ
3. æ€§èƒ½è¾ƒå¥½,éƒ¨ç½²ç®€å•

æ–¹æ¡ˆ 3: çº¯ Rust å®ç°
--------------------
1. ä½¿ç”¨ Rust æœºå™¨å­¦ä¹ åº“ (linfa, smartcore)
2. æˆ–ä½¿ç”¨ tch-rs (PyTorch ç»‘å®š)
3. å®Œå…¨çš„ç±»å‹å®‰å…¨å’Œæ€§èƒ½

æ¨èå¼€å§‹æ–¹å¼:
------------
1. å…ˆç”¨ Python å¿«é€ŸåŸå‹ (æœ¬æ–‡ä»¶)
2. éªŒè¯ç®—æ³•æœ‰æ•ˆå
3. è¿ç§»åˆ° Rust (å¦‚æœéœ€è¦æ€§èƒ½)
"""
    print(guide)


if __name__ == '__main__':
    demo_training()
    integration_guide()
